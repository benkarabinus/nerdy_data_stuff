---
title: "COMP 4442 Midterm, Fall 2021"
author: "Ben Karabinus"
output:
    word_document: default
    html_document: default
    pdf_document: default
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(leaps)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(ggeasy)
library(lawstat)
# Load any additional packages, if any, that you use as part of your answers here
```

There are six questions on this midterm, all of which have multiple parts. Please be sure to provide answers to all parts of each question. Each question has an associated .csv file, which you will load into memory at the beginning of the question. All of the included data sets are simulated, so any results should not be taken as evidence for or against the existence of anything in the real world. The data were simulated to minimize the ambiguity and messiness that typifies real data. If you feel that something is ambiguous in a way that impedes your ability to answer the questions, please let me know. 

I believe in you! 

## Question 1: Basic ANOVA - 10 points total

A tire manufacturing company wants to know if different formulations of tire rubber result in differences in tire durability. They are interested in four different rubber formulations ("form"). To test this, 20 tires of each rubber formulation are selected for testing. Aside from the rubber formulation, all 80 tires in this experiment are otherwise exactly the same. The durability of each tire is tested using a durability machine, which mimics the forces and stress a tire is exposed to when installed in a standard sedan driving down a flat asphalt road at 60 miles per hour. The machines tracks how many miles the tire has "traveled" based on the number of rotations of the tire. The durability test stops when the tire's structure fails, which is when the durability machine records the number of traveled miles ("miles"). The data from this hypothetical experiment is contained in the Q1data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question. 
```{r echo=TRUE}

tires <- read.csv("Q1data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer
# transform rubber formulation to factor
tires$form <- as.factor(tires$form)
# check the data structure
str(tires)

```

# Q1, Part 1: Assessing the normality of groups assumption (2 points)

You will assess the assumption of normality in two ways: quantitatively and visually.

In this first code chunk, please conduct an appropriate *quantitative* assessment of the normality assumption and display the results.
  
```{r echo=TRUE}

# use the Shapiro-Wilk test with dplyr to assess normality and display as tibble
sw_tires <- tires %>% 
                group_by(form)%>%
                summarize(pval=shapiro.test(miles)$p)
sw_tires

```

In this second code chunk, please conduct an appropriate *visual* assessment of the normality assumption and display the visualization/s you create. 

```{r echo=TRUE}

# use ggqqplot to assess the assumption of normality for data by factor level
ggqqplot(tires, x="miles", facet.by = "form")

```

# Q1, Part 2: Assessing the equality of variances of groups assumption (2 points)

You will assess the assumption of equality of variances in two ways: quantitatively and visually.

In this first code chunk, please conduct an appropriate *quantitative* assessment of the equality of variances assumption and display the results.
  
```{r echo=TRUE}

# conduct the Brown-Forsythe test 
levene.test(tires$miles, tires$form)

```

In this second code chunk, please conduct an appropriate *visual* assessment of the equality of variances assumption and display the visualization/s you create. 

```{r echo=TRUE}

# create ggplot geom_boxplot to assess variance between groups visually
ggplot(tires, aes(x=form, y=miles))+
       geom_boxplot()+
       ggtitle("Mean Miles Traveled by Rubber Formulation Method")+
       ggeasy::easy_center_title()

```

# Q1, Part 3: Fitting the ANOVA model (2 points)

Now, you will conduct an ANOVA on the tires data set that can provide an answer to the research question: do different formulations of tire rubber have different durability? Please be sure to display the results of your analysis. 

```{r echo=TRUE}

# create the tires ANOVA
tires.aov <- aov(miles ~ form, data = tires)
# display a summary of the tires ANOVA
summary(tires.aov)

```

# Q1, Part 4: Interpreting the ANOVA results (2 points)

1) What is the null hypothesis being tested by the ANOVA you conducted? Based on the results of your analysis, do you reject or fail to reject this null hypothesis?

Your answer here: 

The null and alternate hypothesis being tested is as follows:

$H_0:$ The mean number of miles traveled before structural failure of tires is the same across the four different types of rubber formulation methods.

$H_a:$ The mean number of miles traveled before structural failure of tires is different for at least one of four rubber formulation methods.

Based on the results of the above ANOVA we fail to reject the null hypothesis "$H_0$" and can conclude that there is not a statistically significant difference in mean miles traveled before structural failure of tires created using any of the four rubber formulation methods.



2) What do the results of your ANOVA suggest about the research question? That is, what is your answer to the tire manufacturer's research question about tire durability? 

Your answer here: 

Rubber formulation method does not have a significant effect on the durability of tires.


## Question 2: Multifactor ANOVA - 10 points 

A health researcher designed an experiment to test the effects of two medications, Lowesterol and Lipidown, on LDL cholesterol levels of people who had been diagnosed as having high cholesterol but no other health problems. He recruited 160 participants, all of whom took two pills each day for 90 days. For 40 participants, both pills were placebos. For 40 participants, one pill contained Lowesterol and the other pill was a placebo. For 40 participants, one pill contained Lipidown and the other pill was a placebo. For the last 40 participants, one pill contained Lowesterol and the other contained Lipidown. After 90 days, each participant gave a blood sample and the LDL level in their blood was recorded. The data from this hypothetical experiment is contained in the Q2data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r echo=TRUE}

drugs <- read.csv("Q2data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

drugs$lowesterol <- as.factor(drugs$lowesterol)
drugs$lipidown <- as.factor(drugs$lipidown)

str(drugs)

```

# Q2, Part 1: Fitting the factorial ANOVA model (4 points)

Now, you will conduct a two-way ANOVA with an interaction on the drug data. Use post.ldl as the outcome. Please be sure to display the results of your analysis. 

```{r echo=TRUE}

# create two-way ANOVA with interaction term
drugs.aov<-aov(post.ldl ~ lowesterol*lipidown,data=drugs)
# print ANOVA summary
summary(drugs.aov)  
  
```

# Q2, Part 2: Interpreting the factorial ANOVA model (6 points)

Use the output from the factorial ANOVA to answer the following three questions.

1) Is the main effect of Lowesterol significant?

Your answer here (yes/no):

No

2) Is the main effect of Lipidown significant?

Your answer here (yes/no):

Yes

3) Is the interaction between Lipidown and Lowesterol significant?

Your answer here (yes/no): 

No

## Question 3: Multiple Regression - 20 points total

A security firm contracted by a shopping center wants to examine the factors that contribute to "loss" (theft of money or goods by customers or employees of a store) in the 200 stores in the shopping center. They have four pieces of information reported by the shopping center about each store: the amount of loss in dollars ("loss", continuous), the area of the store in square feet ("area", continuous), the average number of people who walk into the store on a weekly basis ("traffic", continuous), and whether the store is primarily a retail store (retail=1) or a service-oriented store (retail=0). The data from this hypothetical study is contained in the Q3data.csv file.

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r echo=TRUE}

mall <- read.csv("Q3data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

mall$retail <- as.factor(mall$retail)

str(mall)

```

# Q3, Part 1: Fitting the regression model (2 points)

Now, you will conduct a multiple regression analysis. The outcome for this regression will be loss, and the predictors will be retail, area, and traffic. Be sure to display the results of your analysis. 

```{r echo=TRUE}

# create the "mall.reg" regression model
mall.reg <- lm(loss ~ retail+area+traffic, data=mall)
# print model summary 
summary(mall.reg)

```


# Q3, Part 2: Checking diagnostic plots (4 points)

Please display the diagnostic plots for the model you fit in the previous part of this question and answer the question below: 

```{r echo=TRUE}

# print regression diagnostic plots (mall.reg)
plot(mall.reg)

```

1) What is the most obvious problem that all of the diagnostic plots for this model share? 

Your answer here: 

Each of the diagnostic plots shows that observation 200 can be considered an outlier and it's inclusion in the model might have an undue effect on parameter estimates determined by the model.

2) What would be a good solution to this specific problem?

Your answer here:

In my opinion the solution to the problem with observation 200 is to remove it from the model entirely. That being said, the "loss" for observation 200 is much lower than that of other data. This should be investigated further to justify removal.

# Q3, Part 3: Re-fitting the regression model (4 points)

Now, implement the solution you proposed in the last part and re-fit the regression model. Be sure to display the results of your updated analysis.

```{r echo=TRUE}

# remove observation 200
mall.remove <- mall[-c(200),]
# create the regression model (mall.reg.change)
mall.reg.change <- lm(loss ~ retail+area+traffic, data = mall.remove)
# print the model summary
summary(mall.reg.change)  
  
```

Next, display the updated diagnostic plots and answer the question below

```{r echo=TRUE}

# print diagnostic plots (mall.reg.change)
plot(mall.reg.change)

```

Did your solution to the problem you identified in Q3, Part 2 noticeably improve the diagnostic plots of the model?

Your answer here: 

Yes, removing observation 200 from the data noticeably improved the diagnostic plots when compared to plots generated prior to removal.

# Q3, Part 4: Interpreting the re-fitted regression model (10 points)

3) Interpret the estimated intercept in the context of the predicted outcome and the predictors.

Your answer here:

The intercept can be interpreted as the predicted loss in dollars holding all other variables constant and assuming the store is a service oriented location.

4) Interpret the coefficient associated with retail:

Your answer here:

Holding all other variables constant predicted loss will increase by $\approx$ 4.9477571 if the store is a retail oriented location. 

5) What is the predicted amount of loss for a non-retail store that has an area of 1000 square feet and average weekly traffic of 200?

```{r echo=TRUE}
# alternative method pull coefficients from the model
# intercept <- mall.reg.change$coefficients[1]
# area_coeff <- mall.reg.change$coefficients[3]
# traffic_coeff <- mall.reg.change$coefficients[4]
# intercept
# area_coeff
# traffic_coeff
# predicted_value <- intercept+(1000)*area_coeff+(200)*traffic_coeff
# predicted_value

# create the observation
d_predict <- data.frame(retail = "0", area = 1000, traffic = 200)
# predict sim_tot_fat
predict(mall.reg.change, d_predict)
```

## Question 4: Automated model selection - 35 points total

The data set Q4data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r echo=TRUE}

many.var <- read.csv("Q4data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(many.var)

```

# Q4, Part 1: Forward selection - 10 points

First, you will use forward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 as part of your use of the function. After this, display the model selected using forward selection. 

```{r echo=TRUE}

# check the structure
str(many.var)
# create the null model for forward selection (model)
fwd.null <- lm(y ~ 1, data=many.var)
# sanity check
summary(fwd.null)
# create the scope for forward selection (formula)
fwd.scope <- as.formula("y ~ x1+x2+x3+x4+x5+x6+x7+x8")
# sanity check
fwd.scope
# create the forward selection model
forward.model <- step(fwd.null,scope=fwd.scope,
                      direction="forward",trace=1)

```

The model selected by forward selection:

```{r echo=TRUE}

# print model summary
summary(forward.model)

```


# Q4, Part 2: Backward selection - 10 points

Next, you will use backward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 or trace=TRUE as part of your use of the function. After this, display the model selected using backward selection. 

```{r echo=TRUE}

# create the full model for backward selection (model)
bwd.full <- lm(y ~ ., data=many.var)
# sanity check
summary(bwd.full)
# create the scope for backward selection (formula)
bwd.scope <- as.formula("y ~ 1")
# sanity check
bwd.scope
# creat the backward model
backward.model <- step(bwd.full, scope = bwd.scope,
                       direction = "backward", trace = 1)

```

The model selected by backward selection:

```{r echo=TRUE}

# print model summary
summary(backward.model)

```

# Q4, Part 3: Best subsets selection - 10 points

Finally, you will use best subsets selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. *Be sure to display a table (filled with either stars or TRUE/FALSE values) that shows which predictors were included in the best models of each size* and *display a plot showing the BIC values of the best models of each size*. After this, display the model selected using best subsets selection. 

```{r echo=TRUE}

# create formula for the full model
fmla.full <- as.formula("y ~ x1+x2+x3+x4+x5+x6+x7+x8")
# sanity check
fmla.full
# create the vector of outcomes
y <- many.var$y
# sanity check
y
# create the model matrix
x <- model.matrix(fmla.full, data = many.var)
# sanity check
x
# get model parameters using regsubsets (exhaustive)
# remove intercept (redundant)
best <- regsubsets(x=x[,2:ncol(x)], y=y, method="exhaustive", nvmax=8, nbest=1)
# create the star table
star <- summary(best)
# get boolean vectors of best model for each size
subsetsBoolean<-summary(best)$which
# create visual of best BIC's for models
bic_plt <- qplot(1:length(summary(best)$bic), summary(best)$bic)

# print star table and plot
star
bic_plt


```

The model selected by best subsets selection:

```{r echo=TRUE}

# min function searches BICs returns model number of lowest BIC
best.subset.bic<-which(summary(best)$bic==min(summary(best)$bic))[1] # Min function searches BICs and shows model number
# sanity check
best.subset.bic
# pull dimension (variable) names
varnames <- attr(subsetsBoolean, "dimnames")[[2]]
# get  the best variables
best.varnames <- varnames[subsetsBoolean[best.subset.bic,]]
# print variables
best.varnames
# create the model
best.subsets.model <- lm(y ~ x1+x2+x3+x4+x7, data=many.var)
# print model summary
summary(best.subsets.model)

```


# Q4, Part 4: Comparing models - 5 points

Now that you have conducted three methods of automated model selection on the same data set, please compare the models that were selected by each method. If the methods included different predictors in their final models, list those predictors that were different. 

Please write your answers below:

1) Were there any differences between the forward and backward models?

Your answer here (yes/no):

*If yes*, list the predictors that did not appear in both models:

No

2) Were there any differences between the forward and best subsets models?

Your answer here (yes/no):

*If yes*, list the predictors that did not appear in both models:

Yes, x5 only appeared in the forward selection model.

*x5

3) Were there any differences between the backward and best subsets models?

Your answer here (yes/no): 

*If yes*, list the predictors that did not appear in both models:

Yes, x5 only appeared in the backward selection model.

*x5

## Question 5: Nested model selection - 15 points total 

The data set Q5data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. This is the same data set used in Question 4, but please reload the data set under a new name to ensure no "cross-contamination" between questions. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r echo=TRUE}

Q5.var <- read.csv("Q5data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(Q5.var)

```

# Q5, Part 1: Identifying nested models - 10 points

I fitted five regression models using different sets of predictors. Run the code chunk below to estimate and view the models I fitted. Review the output for these models and answer the questions below.

```{r echo=TRUE}

model.1 = lm(y~x1, data=Q5.var)
model.2 = lm(y~x1+x2, data=Q5.var)
model.3 = lm(y~x1+x3, data=Q5.var)
model.4 = lm(y~x1+x2+x3, data=Q5.var)
model.5 = lm(y~x1+x2+x3+x1:x2+x1:x3+x2:x3+x1:x2:x3, data=Q5.var)

summary(model.1)
summary(model.2)
summary(model.3)
summary(model.4)
summary(model.5)


```

1) If Model 5 (model.5) is considered to be the "full model", which of the remaining models - Models 1, 2, 3, and 4 - are nested relative to it? 

Your answer here:

Models 1, 2, 3 and 4 are nested relative to model 5. Models 1, 2, 3 and 4 can be specified by setting parameters of model 5 to zero.

2) If Model 4 (model.4) is considered to be the "full model", which of the remaining models - Models 1, 2, and 3 - are nested relative to it? 

Your answer here:

Models 1, 2 and 3 are nested relative to model 4. Models 1, 2 and 3 can be specified by setting parameters of model 4 to zero.

3) If Model 3 (model.3) is considered to be the "full model", which of the remaining models - Models 1 and 2 - are nested relative to it? 

Your answer here:

Model 1 is nested relative to model 3. Model 1 can be specified by setting parameters of model 3 to zero.

4) In the code chunk below, specify a new model that is nested relative to Model 5 AND in which Model 2 is nested. That is, specify a model that fits the nested model relationship depicted below:

Model 5 (7 predictor coefficients) <-- (Your model, 3-6 predictor coefficients) <-- Model 2 (2 predictor coefficients)

Please note that you cannot chose any of the models already fitted in this question. You must specify a model that hasn't yet been fitted. 

```{r echo=TRUE}

# create the model
model.new <- lm(y~x1+x2+x3+x1:x2+x1:x3+x2:x3, data=Q5.var)
# print model summary  
summary(model.new)

```

# Q5, Part 2: Nested model testing - 5 points

For this part, you will conduct two nested model tests. In the first test, you will test Model 2 and the new model you specified. In the second test, you will test the new model you specified and Model 5. After you've done this, answer the two questions below. 

```{r echo=TRUE}

# conduct nested model test
m2.vs.new <- anova(model.2, model.new)
# pritn results  
m2.vs.new

```

```{r echo=TRUE}

# conduct the nested model test
new.vs.m5 <- anova(model.new, model.5)
# print the results  
new.vs.m5

```

6) Based on the result of the test between Model 2 and your new model, which model would you choose?

Your answer here:

I would choose the new model. With a $p-value \approx 0.008$ we can reject the null hypothesis and accept the alternate, that model.new accounts for significantly more variability in the outcome than model.2. 

7) Based on the result of the test between your new model and Model 5, which model would you choose? 

Your answer here:

I would choose the new model. We fail to reject the null hypothesis. Model.5 does not account for significantly more variability in the outcome than model.new.

Note:

In practice use of model.5 and it's additional predictors might depend upon domain knowledge, the goal of the analysis, etc.


## Question 6: Basic logistic regression - 10 points total

A state public health agency wants to investigate the presence of dangerous amounts of lead in drinking water across households within the state. Investigators collected tap water samples from 150 single-family homes and obtained information about each house. Based on advice from an environmental agency, the investigators classified a tap water sample as being safe if it had levels below 15 parts per billion (0) or potentially dangerous if it had levels equal to or greater than 15 parts per billion (1). In addition, they tested the "hardness" (i.e, presence of dissolved calcium, magnesium, and other minerals) of the water sample, which they categorized as being low (0) or high (1). They also noted the age of the house in years and the location type of the house (urban, suburban, or rural). The data from this hypothetical study is contained in the Q6data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question
```{r echo=TRUE}

lead <- read.csv("Q6data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(lead)

```

# Q6, Part 1: Fitting a logistic model - 5 points

Fit a logistic regression model using "danger" (categorical) as the outcome and "age" (continuous), "loc" (categorical), and "hard" (categorical) as predictors. Be sure to display the results of the analysis.

```{r echo=TRUE}

# change hard to categorical variable
lead$hard <- as.factor(lead$hard)
# check the data structure
str(lead)
# fit the model
danger.model <- glm(danger ~ age+loc+hard,
                    data = lead,family="binomial")

#print the model summary
summary(danger.model)

```

# Q6, Part 2: Interpreting a logistic model - 5 points

1) Based on the results of your analyses, which predictor coefficients were significantly different from zero? There is at least one. 

Your answer here:

The results of the analysis show that the following predictor coefficients were significant:

*age

2) Of the statistically significant predictor/s you identified in the first sub-question, which predictor/s (if any) indicate that the presence of a dangerous level of lead is *more likely* as the value of the predictor increases? Which predictors (if any) indicate that that the presences of a dangerous level of lead is *less likely* as the value of the predictor increases? 

More likely as values of predictor/s increase/s (your answer here): 

Of the statistically significant predictor coefficients identified by the analysis, "age" (the age of the home) is the only predictor coefficient that indicates an increase in the probability of dangerous lead levels as the predictor increases.

Less likely as values of predictor/s increase/s (your answer here):

No statistically significant predictors indicated a decrease in likelihood of dangerous lead levels as the predictor increases.
