ggplot(data = dat.XG, aes(x=UnEmpRate))+geom_density(fill = 'purple')+
xlab('Unemployment Rate (%)')
ggplot(data = dat.XG, aes(x=Value))+geom_density(fill = 'green')+
xlab('Zillow Home Value Estimate (USD)')
ggcorr(dat.XG, label = TRUE)+ggtitle('Pearson Correlation')+
easy_center_title()
# visual assessment distribution of home values by size (histogram)
ggplot(data = dat.XG, aes(y=Value, group = Size, fill = Size))+
geom_boxplot()+
theme_classic()+
ylab("Value (USD)")+
facet_wrap(~Size)
# quantitative assessment of home values by size
dat.XG %>% group_by(Size)%>% summarize(mean(Value), median(Value))
# split data into training and test sets
n <- nrow(dat.XG)
# shuffle data
dat.XG <- dat.XG[sample(1:n),]
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (1495)
xg.train <- dat.XG[tv.split==1,]
# test set (640)
xg.test <- dat.XG[tv.split == 0,]
# create matrix of predictors
X.train <- data.matrix(xg.train[, -7])
X.test <- data.matrix(xg.test[, -7])
# create vector of outcomes
Y.train <- data.matrix(xg.train[, 7])
Y.test <- data.matrix(xg.test[, 7])
# create xgb.DMatrix for training and test
xgb_train <- xgb.DMatrix(data = X.train, label = Y.train)
xgb_test <- xgb.DMatrix(data = X.test, label = Y.test)
# define the pre-processing recipe, how data will be processed prior to model tuning
recipe <-
recipes::recipe(Value ~ ., data = xg.train) %>%
# convert categorical variables to factors
recipes::step_string2factor(all_nominal()) %>%
prep()
# create folds for cross validation
cv_folds <-
# treat data using recipe
recipes::bake(
recipe,
# specify data for CV (training data)
new_data = xg.train
) %>%
# specify num_folds
rsample::vfold_cv(v = 7)
# create folds for cross validation
cv_folds <-
# treat data using recipe
recipes::bake(
recipe,
# specify data for CV (training data)
new_data = xg.train
) %>%
# specify num_folds
rsample::vfold_cv(v = 5)
# XGBoost model specification using parsnip library, boost_tree function
model.XG <-
parsnip::boost_tree(
mode = "regression",
trees = tune(),
min_n = tune(), # min number of data points required for split in tree node
tree_depth = tune(), # max splits for trees
learn_rate = tune(), # rate at which model adapts at each iteration
loss_reduction = tune(), # min reduction in loss function required for split
# stop tuning if no improvment after 100 iterations
sample_size = 0.25
) %>%
# specify xgboost algorithm as engine, objective function regression
set_engine("xgboost", objective = "reg:squarederror")
# specify parameters for grid search
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction(),
trees()
)
# set grid space
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 100 # (5 CV folds) * (size = 100), 500 parameter combinations
)
# print head for search grid
knitr::kable(head(xgboost_grid))
# define the workflow f
xgboost_wf <-
workflows::workflow() %>%
add_model(model.XG) %>% # add the XGBoost model defined with parsnip
add_formula(Value ~ .) # specify formula
# tune the model using tidymodels tune()
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf, # specified workflow
resamples = cv_folds, # specified cross-validation
grid = xgboost_grid, # specified search grid
metrics = yardstick::metric_set(rmse), # metric for tuning,rmse only
control = tune::control_grid(verbose = TRUE)
)
(sum(222000, 172000,-128000))^2
# select and view best parameters from model tuning
best_params <- xgboost_tuned %>%
tune::select_best("rmse")
knitr::kable(best_params)
# finalize the model
model.best <- model.XG %>%
finalize_model(best_params)
model.best
# use predifined recipe to prepare the training data
train <- bake(recipe, new_data = xg.train)
train_prediction <- model.best %>% # fit the best model on the training data
fit(formula = Value ~ ., data = train) %>%
predict(train) %>% # make prediction on training data using the best model
bind_cols(xg.train) # bind predict with xg.train for side by side view
# print results
train_prediction
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(Value, .pred) %>%
mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
test_processed  <- bake(recipe, new_data = xg.test)
test_prediction <- model.best %>%
# fit the model on all the training data
fit(
formula = Value ~ .,
data    = train
) %>%
# use the training model fit to predict the test data
predict(new_data = test_processed) %>%
bind_cols(xg.test)
# measure the accuracy of our model using `yardstick`
xgboost_score <-
test_prediction %>%
yardstick::metrics(Value, .pred)
knitr::kable(xgboost_score)
# create xgb.Booster object to use for visualizing variable importance
params <- list(
eta = model.best$args$learn_rate[2],
max_depth = model.best$args$tree_depth[2],
gamma = model.best$args$loss_reduction[2],
min_child_weight = model.best$args$min_n[2],
subsample = 0.5
)
xgb.fit <- xgboost(
params = params,
data = xgb_train,
objective = 'reg:squarederror',
nrounds = 861,
verbose = 0
)
importance_matrix <- xgb.importance(model = xgb.fit)
# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
# create xgb.Booster object to use for visualizing variable importance
params <- list(
eta = model.best$args$learn_rate[2],
max_depth = model.best$args$tree_depth[2],
gamma = model.best$args$loss_reduction[2],
min_child_weight = model.best$args$min_n[2],
subsample = 0.25
)
xgb.fit <- xgboost(
params = params,
data = xgb_train,
objective = 'reg:squarederror',
nrounds = 861,
verbose = 0
)
importance_matrix <- xgb.importance(model = xgb.fit)
# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
View(model.best)
View(xgboost_tuned)
View(xgboost_tuned)
knitr::opts_chunk$set(echo = TRUE)
# load necessary libraries
library(dplyr)
library(tidyverse)
library(stringr)
library(ggplot2)
library(ggeasy)
if(!require(GGally)){ install.packages("GGally"); library(GGally)}
if(!require(skimr)){ install.packages("skimr"); library(skimr)}
if(!require(xgboost)){ install.packages("xgboost"); library(xgboost)}
if(!require(caret)){ install.packages("caret"); library(caret)}
if(!require(recipes)){ install.packages("recipes"); library(recipes)}
if(!require(rsample)){ install.packages("rsample"); library(rsample)}
if(!require(parsnip)){ install.packages("parsnip"); library(parsnip)}
if(!require(dials)){ install.packages("dials"); library(dials)}
if(!require(workflows)){ install.packages("workflows"); library(workflows)}
if(!require(yardstick)){ install.packages("yardstick"); library(yardstick)}
if(!require(tune)){ install.packages("tune"); library(tune)}
if(!require(doParallel)){ install.packages("doParallel");library(doParallel)}
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
# load data
condos <- read.csv("Metro_zhvi_condos and co-ops.csv", header=TRUE, sep=",")
fiveBed <- read.csv("Metro_zhvi_Five_Plus_Bedroom.csv", header=TRUE, sep=",")
fourBed <- read.csv("Metro_zhvi_Four_Bedroom.csv", header=TRUE, sep=",")
threeBed <- read.csv("Metro_zhvi_Three_Bedroom.csv", header=TRUE, sep=",")
twoBed <- read.csv("Metro_zhvi_Two_Bedroom.csv", header=TRUE, sep=",")
oneBed <- read.csv("Metro_zhvi_One_Bedroom.csv", header=TRUE, sep=",")
popIncome <- read.csv("PopulationandPerCapitaIncomeMSA2019.csv", header=TRUE, sep=",")
Rpp <- read.csv("RegionalPriceParitiesMSA2019.csv", header=TRUE, sep=",")
unEmp <- read.csv("UnemploymentMSA2019.csv", header=TRUE, sep=",")
# ZHVI data structure represents each month in a year as a column
# keep only 2019
condos <- subset(condos, select = -c(1:2, 4, 18:38))
fiveBed <- subset(fiveBed, select = -c(1:2, 4, 18:38))
fourBed <- subset(fourBed, select = -c(1:2, 4, 18:38))
threeBed <- subset(threeBed, select = -c(1:2, 4, 18:38))
twoBed <- subset(twoBed, select = -c(1:2, 4, 18:38))
oneBed <- subset(oneBed, select = -c(1:2, 4, 18:38))
# create aggregate of home value for year 2019 (median)
condos$value <- apply(condos[,3:14], 1, median)
fiveBed$value <- apply(fiveBed[,3:14], 1, median)
fourBed$value <- fourBed$value <- apply(fourBed[,3:14], 1, median)
threeBed$value <- threeBed$value <- apply(threeBed[,3:14], 1, median)
twoBed$value <- twoBed$value <- apply(twoBed[,3:14], 1, median)
oneBed$value <- oneBed$value <- apply(oneBed[,3:14], 1, median)
# drop unecessary columns
condos <- subset(condos, select = -c(3:14))
fiveBed <- subset(fiveBed, select = -c(3:14))
fourBed <- subset(fourBed, select = -c(3:14))
threeBed <- subset(threeBed, select = -c(3:14))
twoBed <- subset(twoBed, select = -c(3:14))
oneBed <- subset(oneBed, select = -c(3:14))
# create categorical variable for home size
condos[, "Size"] <- "condo"
fiveBed[, "Size"] <- "five"
fourBed[, "Size"] <- "four"
threeBed[, "Size"] <- "three"
twoBed[, "Size"] <- "two"
oneBed[, 'Size'] <- "one"
# combine ZHVI data
homeValue <- rbind(condos,oneBed, twoBed, threeBed, fourBed, fiveBed)
# drop aggregate (United States)
homeValue <- subset(homeValue, RegionName != "United States")
# rename value to Value
colnames(homeValue)[3] <- "Value"
# clean unemployment data
# remove extra strings in MSA column
unEmp <- unEmp %>% filter(!grepl("Metropolitan NECTA", MSA))
unEmp$MSA <- str_remove_all(unEmp$MSA,
"Metropolitan Statistical Area")
# remove unwanted characters from column MSA
unEmp$State <- sub(".*,", "",unEmp$MSA)
unEmp$MSA <- sub("-.*", "",unEmp$MSA)
unEmp$State <- sub("-.*", "", unEmp$State)
unEmp$MSA <- sub(",.*","",unEmp$MSA)
unEmp$MSA <- str_trim(unEmp$MSA, side = 'both')
unEmp$State <- str_trim(unEmp$State, side = 'both')
# create clean column of MSA's rename to Region
unEmp$Region <- str_c(unEmp$MSA, ", ", unEmp$State)
# drop unneeded columns
unEmp <- subset(unEmp, select = -c(1, 3))
# rename column for unemployment rate
colnames(unEmp)[1] <- "UnEmpRate"
# Drop column GeoFips
Rpp <- subset(Rpp, select = -c(1))
# rename column X2019 and GeoName
colnames(Rpp)[4] <- c("RPP")
colnames(Rpp)[1] <- c("MSA")
# keep aggregate of regional price parities only
Rpp <- subset(Rpp, LineCode == 1)
# remove rows containing United States
Rpp <- Rpp %>% filter(!grepl("United States", MSA))
# clean the MSA column
Rpp$MSA <- str_remove_all(Rpp$MSA, "\\s*\\([^\\)]+\\)")
Rpp$MSA <- str_remove_all(Rpp$MSA, "2/")
Rpp$MSA <- str_trim(Rpp$MSA, side = 'both')
# remove unecessary columns
Rpp <- subset(Rpp, select = -c(2, 3))
# drop NA's
popIncome <- popIncome %>% drop_na()
# drop unnecessary columns
popIncome <- subset(popIncome, select = -c(1))
# drop rows containing "United States" (aggregates)
popIncome <- popIncome %>% filter(!grepl("United States", GeoName))
# rename columns
colnames(popIncome)[1] <- c("MSA")
# split population and per capita income into seperate columns
Pop <- popIncome[popIncome$LineCode == 2, ]
Income <- popIncome[popIncome$LineCode == 3, ]
# rename columns
colnames(Pop)[4] <- c("POP")
colnames(Income)[4] <- c("Income")
# drop unecessary columns
Pop <- subset(Pop, select = -c(2, 3))
Income <- subset(Income, select = -c(2,3))
# create columns for population and income
dat.temp <- inner_join(Pop, Income, by = "MSA")
# clean the MSA column
dat.temp$MSA = str_remove_all(dat.temp$MSA, "\\*")
dat.temp$MSA <- str_remove_all(dat.temp$MSA, "\\s*\\([^\\)]+\\)")
dat.temp$MSA <- str_trim(dat.temp$MSA, side = 'both')
# join RPP and popIncome data
combined <- inner_join(dat.temp, Rpp, by = "MSA")
# clean MSA column combined dataframe and rename Region
combined$State <- sub(".*,", "",combined$MSA)
combined$MSA <- sub("-.*", "",combined$MSA)
combined$State <- sub("-.*", "", combined$State)
combined$MSA <- sub("/.*", "",combined$MSA)
combined$MSA <- sub(",.*","",combined$MSA)
combined$MSA <- str_trim(combined$MSA, side = 'both')
combined$State <- str_trim(combined$State, side = 'both')
combined$Region <- str_c(combined$MSA, ", ", combined$State)
# clean homeValue RegionName and rename Region
homeValue$RegionName <- sub(",.*", "",homeValue$RegionName)
homeValue$RegionName <- sub("-.*", "", homeValue$RegionName)
homeValue$RegionName <- str_trim(homeValue$RegionName, side = 'both')
homeValue$StateName <- str_trim(homeValue$StateName, side = 'both')
homeValue$Region <- str_c(homeValue$RegionName, ", ", homeValue$StateName)
# join homeValue and combined rename dat.XG
dat.XG <- left_join(homeValue, combined, by = "Region")
dat.XG <- dat.XG %>% drop_na()
# join unEmp and dat.XG
dat.XG <- left_join(unEmp, dat.XG, by = "Region")
# drop unnecessary rows and rearrange columns
dat.XG <- subset(dat.XG, select = -c(3, 4, 7, 11))
dat.XG <- dat.XG[c("Region", "POP", "Size", "Income",
"RPP","UnEmpRate", "Value")]
dat.XG$Region <- as.factor(dat.XG$Region)
dat.XG$Size <- as.factor(dat.XG$Size)
# check for NA's in each column
colSums((is.na(dat.XG)))
# examine NA's
which(is.na(dat.XG), arr.ind = T)
#drop NA's
dat.XG <- dat.XG %>% drop_na()
# quick view of summary statistics for data set
skim(dat.XG)
ggplot(data = dat.XG, aes(x=POP))+geom_density(fill = 'red')+
xlab("Population")
ggplot(data = dat.XG, aes(x=Income))+geom_density(fill = 'blue')+
xlab('Income (USD)')
ggplot(data = dat.XG, aes(x=RPP))+geom_density(fill = 'orange')+
xlab('Regioanl Price Parity (% of National Price Level for Consumer Goods)')
ggplot(data = dat.XG, aes(x=UnEmpRate))+geom_density(fill = 'purple')+
xlab('Unemployment Rate (%)')
ggplot(data = dat.XG, aes(x=Value))+geom_density(fill = 'green')+
xlab('Zillow Home Value Estimate (USD)')
ggcorr(dat.XG, label = TRUE)+ggtitle('Pearson Correlation')+
easy_center_title()
# visual assessment distribution of home values by size (histogram)
ggplot(data = dat.XG, aes(y=Value, group = Size, fill = Size))+
geom_boxplot()+
theme_classic()+
ylab("Value (USD)")+
facet_wrap(~Size)
# quantitative assessment of home values by size
dat.XG %>% group_by(Size)%>% summarize(mean(Value), median(Value))
# split data into training and test sets
n <- nrow(dat.XG)
# shuffle data
dat.XG <- dat.XG[sample(1:n),]
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (1495)
xg.train <- dat.XG[tv.split==1,]
# test set (640)
xg.test <- dat.XG[tv.split == 0,]
# create matrix of predictors
X.train <- data.matrix(xg.train[, -7])
X.test <- data.matrix(xg.test[, -7])
# create vector of outcomes
Y.train <- data.matrix(xg.train[, 7])
Y.test <- data.matrix(xg.test[, 7])
# create xgb.DMatrix for training and test
xgb_train <- xgb.DMatrix(data = X.train, label = Y.train)
xgb_test <- xgb.DMatrix(data = X.test, label = Y.test)
# define the pre-processing recipe, how data will be processed prior to model tuning
recipe <-
recipes::recipe(Value ~ ., data = xg.train) %>%
# convert categorical variables to factors
recipes::step_string2factor(all_nominal()) %>%
prep()
# define the pre-processing recipe, how data will be processed prior to model tuning
recipe <-
recipes::recipe(Value ~ ., data = xg.train) %>%
# convert categorical variables to factors
recipes::step_string2factor(all_nominal()) %>%
prep()
# create folds for cross validation
cv_folds <-
# treat data using recipe
recipes::bake(
recipe,
# specify data for CV (training data)
new_data = xg.train
) %>%
# specify num_folds
rsample::vfold_cv(v = 5)
# XGBoost model specification using parsnip library, boost_tree function
model.XG <-
parsnip::boost_tree(
mode = "regression",
trees = tune(),
min_n = tune(), # min number of data points required for split in tree node
tree_depth = tune(), # max splits for trees
learn_rate = tune(), # rate at which model adapts at each iteration
loss_reduction = tune(), # min reduction in loss function required for split
# stop tuning if no improvment after 100 iterations
sample_size = 0.5,
stop_iter = 10
) %>%
# specify xgboost algorithm as engine, objective function regression
set_engine("xgboost", objective = "reg:squarederror")
# specify parameters for grid search
# grid specification
xgboost_params <-
dials::parameters(
min_n(),
tree_depth(),
learn_rate(),
loss_reduction(),
trees()
)
# set grid space
xgboost_grid <-
dials::grid_max_entropy(
xgboost_params,
size = 100 # (5 CV folds) * (size = 100), 500 parameter combinations
)
# print head for search grid
knitr::kable(head(xgboost_grid))
# define the workflow f
xgboost_wf <-
workflows::workflow() %>%
add_model(model.XG) %>% # add the XGBoost model defined with parsnip
add_formula(Value ~ .) # specify formula
# tune the model using tidymodels tune()
xgboost_tuned <- tune::tune_grid(
object = xgboost_wf, # specified workflow
resamples = cv_folds, # specified cross-validation
grid = xgboost_grid, # specified search grid
metrics = yardstick::metric_set(rmse), # metric for tuning,rmse only
control = tune::control_grid(verbose = TRUE)
)
# select and view best parameters from model tuning
best_params <- xgboost_tuned %>%
tune::select_best("rmse")
knitr::kable(best_params)
# select and view best parameters from model tuning
best_params <- xgboost_tuned %>%
tune::select_best("rmse")
best_params
# finalize the model
model.best <- model.XG %>%
finalize_model(best_params)
model.best
# use predifined recipe to prepare the training data
train <- bake(recipe, new_data = xg.train)
train_prediction <- model.best %>% # fit the best model on the training data
fit(formula = Value ~ ., data = train) %>%
predict(train) %>% # make prediction on training data using the best model
bind_cols(xg.train) # bind predict with xg.train for side by side view
# print results
train_prediction
xgboost_score_train <-
train_prediction %>%
yardstick::metrics(Value, .pred) %>%
mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
xgboost_score_train
test_processed  <- bake(recipe, new_data = xg.test)
test_prediction <- model.best %>%
# fit the model on all the training data
fit(
formula = Value ~ .,
data    = train
) %>%
# use the training model fit to predict the test data
predict(new_data = test_processed) %>%
bind_cols(xg.test)
# measure the accuracy of our model using `yardstick`
xgboost_score <-
test_prediction %>%
yardstick::metrics(Value, .pred)
xgboost_score
# create xgb.Booster object to use for visualizing variable importance
params <- list(
eta = model.best$args$learn_rate[2],
max_depth = model.best$args$tree_depth[2],
gamma = model.best$args$loss_reduction[2],
min_child_weight = model.best$args$min_n[2],
subsample = 0.5
)
xgb.fit <- xgboost(
params = params,
data = xgb_train,
objective = 'reg:squarederror',
nrounds = 931,
verbose = 0
)
importance_matrix <- xgb.importance(model = xgb.fit)
# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
