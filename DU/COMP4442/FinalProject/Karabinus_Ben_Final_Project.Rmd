---
title: "COMP 4442 Final Project"
author: "Ben Karabinus"
date: "11/8/2021"
output:
    word_document: default
    html_document: default
    pdf_document: default
---

```{r setup, include=TRUE}

knitr::opts_chunk$set(echo = TRUE)
# load necessary libraries
library(dplyr)
library(tidyverse)
library(stringr)
library(ggplot2)
library(ggeasy)
if(!require(GGally)){ install.packages("GGally"); library(GGally)}
if(!require(skimr)){ install.packages("skimr"); library(skimr)}
if(!require(xgboost)){ install.packages("xgboost"); library(xgboost)}
if(!require(caret)){ install.packages("caret"); library(caret)}
if(!require(recipes)){ install.packages("recipes"); library(recipes)}
if(!require(rsample)){ install.packages("rsample"); library(rsample)}
if(!require(parsnip)){ install.packages("parsnip"); library(parsnip)}
if(!require(dials)){ install.packages("dials"); library(dials)}
if(!require(workflows)){ install.packages("workflows"); library(workflows)}
if(!require(yardstick)){ install.packages("yardstick"); library(yardstick)}
if(!require(tune)){ install.packages("tune"); library(tune)}
if(!require(vip)){ install.packages("vip"); library(vip)}
if(!require(doParallel)){ install.packages("doParallel");library(doParallel)}
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
options(scipen = 9999)
```

```{r echo=TRUE}
# load data 
condos <- read.csv("Metro_zhvi_condos and co-ops.csv", header=TRUE, sep=",")
fiveBed <- read.csv("Metro_zhvi_Five_Plus_Bedroom.csv", header=TRUE, sep=",")
fourBed <- read.csv("Metro_zhvi_Four_Bedroom.csv", header=TRUE, sep=",")
threeBed <- read.csv("Metro_zhvi_Three_Bedroom.csv", header=TRUE, sep=",")
twoBed <- read.csv("Metro_zhvi_Two_Bedroom.csv", header=TRUE, sep=",")
oneBed <- read.csv("Metro_zhvi_One_Bedroom.csv", header=TRUE, sep=",")
popIncome <- read.csv("PopulationandPerCapitaIncomeMSA2019.csv", header=TRUE, sep=",")
Rpp <- read.csv("RegionalPriceParitiesMSA2019.csv", header=TRUE, sep=",")
unEmp <- read.csv("UnemploymentMSA2019.csv", header=TRUE, sep=",")

```

```{r echo=TRUE}

# ZHVI data structure represents each month in a year as a column
# keep only 2019
condos <- subset(condos, select = -c(1:2, 4, 18:38))
fiveBed <- subset(fiveBed, select = -c(1:2, 4, 18:38))
fourBed <- subset(fourBed, select = -c(1:2, 4, 18:38))
threeBed <- subset(threeBed, select = -c(1:2, 4, 18:38))
twoBed <- subset(twoBed, select = -c(1:2, 4, 18:38))
oneBed <- subset(oneBed, select = -c(1:2, 4, 18:38))

```


```{r echo=TRUE}

# create aggregate of home value for year 2019 (median)
condos$value <- apply(condos[,3:14], 1, median)
fiveBed$value <- apply(fiveBed[,3:14], 1, median)
fourBed$value <- fourBed$value <- apply(fourBed[,3:14], 1, median)
threeBed$value <- threeBed$value <- apply(threeBed[,3:14], 1, median)
twoBed$value <- twoBed$value <- apply(twoBed[,3:14], 1, median)
oneBed$value <- oneBed$value <- apply(oneBed[,3:14], 1, median)

```



```{r echo=TRUE}

# drop unecessary columns
condos <- subset(condos, select = -c(3:14))
fiveBed <- subset(fiveBed, select = -c(3:14))
fourBed <- subset(fourBed, select = -c(3:14))
threeBed <- subset(threeBed, select = -c(3:14))
twoBed <- subset(twoBed, select = -c(3:14))
oneBed <- subset(oneBed, select = -c(3:14))
# create categorical variable for home size
condos[, "Size"] <- "condo"
fiveBed[, "Size"] <- "five"
fourBed[, "Size"] <- "four"
threeBed[, "Size"] <- "three"
twoBed[, "Size"] <- "two"
oneBed[, 'Size'] <- "one"
# combine ZHVI data
homeValue <- rbind(condos,oneBed, twoBed, threeBed, fourBed, fiveBed)
# drop aggregate (United States)
homeValue <- subset(homeValue, RegionName != "United States")
# rename value to Value
colnames(homeValue)[3] <- "Value"

```

```{r echo=TRUE}

# clean unemployment data
# remove extra strings in MSA column
unEmp <- unEmp %>% filter(!grepl("Metropolitan NECTA", MSA))
unEmp$MSA <- str_remove_all(unEmp$MSA, 
                            "Metropolitan Statistical Area")
# remove unwanted characters from column MSA
unEmp$State <- sub(".*,", "",unEmp$MSA)
unEmp$MSA <- sub("-.*", "",unEmp$MSA)
unEmp$State <- sub("-.*", "", unEmp$State)
unEmp$MSA <- sub(",.*","",unEmp$MSA)
unEmp$MSA <- str_trim(unEmp$MSA, side = 'both')
unEmp$State <- str_trim(unEmp$State, side = 'both')
# create clean column of MSA's rename to Region
unEmp$Region <- str_c(unEmp$MSA, ", ", unEmp$State)
# drop unneeded columns
unEmp <- subset(unEmp, select = -c(1, 3))
# rename column for unemployment rate
colnames(unEmp)[1] <- "UnEmpRate"

```


```{r echo=TRUE}

# Drop column GeoFips
Rpp <- subset(Rpp, select = -c(1))
# rename column X2019 and GeoName
colnames(Rpp)[4] <- c("RPP")
colnames(Rpp)[1] <- c("MSA")
# keep aggregate of regional price parities only
Rpp <- subset(Rpp, LineCode == 1) 
# remove rows containing United States
Rpp <- Rpp %>% filter(!grepl("United States", MSA))
# clean the MSA column
Rpp$MSA <- str_remove_all(Rpp$MSA, "\\s*\\([^\\)]+\\)")
Rpp$MSA <- str_remove_all(Rpp$MSA, "2/")
Rpp$MSA <- str_trim(Rpp$MSA, side = 'both')
# remove unecessary columns
Rpp <- subset(Rpp, select = -c(2, 3))

```

```{r echo=TRUE}

# drop NA's
popIncome <- popIncome %>% drop_na()
# drop unnecessary columns
popIncome <- subset(popIncome, select = -c(1))
# drop rows containing "United States" (aggregates)
popIncome <- popIncome %>% filter(!grepl("United States", GeoName))
# rename columns 
colnames(popIncome)[1] <- c("MSA")
# split population and per capita income into seperate columns
Pop <- popIncome[popIncome$LineCode == 2, ]
Income <- popIncome[popIncome$LineCode == 3, ]
# rename columns
colnames(Pop)[4] <- c("POP")
colnames(Income)[4] <- c("Income")
# drop unecessary columns
Pop <- subset(Pop, select = -c(2, 3))
Income <- subset(Income, select = -c(2,3))
# create columns for population and income
dat.temp <- inner_join(Pop, Income, by = "MSA")
# clean the MSA column 
dat.temp$MSA = str_remove_all(dat.temp$MSA, "\\*")
dat.temp$MSA <- str_remove_all(dat.temp$MSA, "\\s*\\([^\\)]+\\)")
dat.temp$MSA <- str_trim(dat.temp$MSA, side = 'both')
# join RPP and popIncome data
combined <- inner_join(dat.temp, Rpp, by = "MSA")

```

```{r echo=TRUE}

# clean MSA column combined dataframe and rename Region
combined$State <- sub(".*,", "",combined$MSA)
combined$MSA <- sub("-.*", "",combined$MSA)
combined$State <- sub("-.*", "", combined$State)
combined$MSA <- sub("/.*", "",combined$MSA)
combined$MSA <- sub(",.*","",combined$MSA)
combined$MSA <- str_trim(combined$MSA, side = 'both')
combined$State <- str_trim(combined$State, side = 'both')
combined$Region <- str_c(combined$MSA, ", ", combined$State)
# clean homeValue RegionName and rename Region
homeValue$RegionName <- sub(",.*", "",homeValue$RegionName)
homeValue$RegionName <- sub("-.*", "", homeValue$RegionName)
homeValue$RegionName <- str_trim(homeValue$RegionName, side = 'both')
homeValue$StateName <- str_trim(homeValue$StateName, side = 'both')
homeValue$Region <- str_c(homeValue$RegionName, ", ", homeValue$StateName)

# join homeValue and combined rename dat.XG
dat.XG <- left_join(homeValue, combined, by = "Region")
dat.XG <- dat.XG %>% drop_na()
# join unEmp and dat.XG
dat.XG <- left_join(unEmp, dat.XG, by = "Region")
# drop unnecessary rows and rearrange columns 
dat.XG <- subset(dat.XG, select = -c(3, 4, 7, 11))
dat.XG <- dat.XG[c("Region", "POP", "Size", "Income", 
                         "RPP","UnEmpRate", "Value")]
# transform variable types
dat.XG$Region <- as.factor(dat.XG$Region)
dat.XG$Size <- as.factor(dat.XG$Size)
# check for NA's in each column
colSums((is.na(dat.XG)))
# examine NA's
which(is.na(dat.XG), arr.ind = T)
#drop NA's
dat.XG <- dat.XG %>% drop_na()

```

##############Exploratory data analysis###########

```{r echo=TRUE}

# quick view of summary statistics for data set
skim(dat.XG)

```


```{r echo=TRUE}

# plot the distribution on continous variables
ggplot(data = dat.XG, aes(x=POP))+geom_density(fill = 'red')+
  xlab("Population")
ggplot(data = dat.XG, aes(x=Income))+geom_density(fill = 'blue')+
  xlab('Income (USD)')
ggplot(data = dat.XG, aes(x=RPP))+geom_density(fill = 'orange')+
  xlab('Regioanl Price Parity (% of National Price Level for Consumer Goods)')
ggplot(data = dat.XG, aes(x=UnEmpRate))+geom_density(fill = 'purple')+
  xlab('Unemployment Rate (%)')
ggplot(data = dat.XG, aes(x=Value))+geom_density(fill = 'green')+
  xlab('Zillow Home Value Estimate (USD)')

```



```{r echo=TRUE}

# scatter plots of "Value" as function of continuous predictors
ggplot(data = dat.XG, aes(x=POP, y=Value))+geom_point(color = 'red')+
  xlab("Population")
ggplot(data = dat.XG, aes(x=Income, y= Value))+geom_point(color = 'blue')+
  xlab('Income (USD)')
ggplot(data = dat.XG, aes(x=RPP, y=Value))+geom_point(color = 'orange')+
  xlab('Regioanl Price Parity (% of National Price Level for Consumer Goods)')
ggplot(data = dat.XG, aes(x=UnEmpRate, y=Value))+geom_point(color = 'purple')+
  xlab('Unemployment Rate (%)')

```


```{r echo=TRUE}

# distribution of home values by size (histogram)
ggplot(data = dat.XG, aes(y=Value, group = Size, fill = Size))+
  geom_boxplot()+
  theme_classic()+
  ylab("Value (USD)")+
  facet_wrap(~Size)

```


```{r echo=TRUE}
# quantitative assessment of home values by size
dat.XG %>% group_by(Size)%>% summarize(mean(Value), median(Value))

```


```{r echo=TRUE}

# split data into training and test sets
n <- nrow(dat.XG)
# shuffle data
dat.XG <- dat.XG[sample(1:n),]
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification    
table(tv.split)   
# training set (1495)
xg.train <- dat.XG[tv.split==1,]
# test set (640)
xg.test <- dat.XG[tv.split == 0,]

```


```{r echo=TRUE}

###### Alternaitve Method for Data Prep ######
# create matrix of predictors
#X.train <- data.matrix(xg.train[, -7])
#X.test <- data.matrix(xg.test[, -7])
# create vector of outcomes 
#Y.train <- data.matrix(xg.train[, 7])
#Y.test <- data.matrix(xg.test[, 7])
# create xgb.DMatrix for training and test
#xgb_train <- xgb.DMatrix(data = X.train, label = Y.train)
#xgb_test <- xgb.DMatrix(data = X.test, label = Y.test)

```


```{r echo=TRUE}

# define pre-processing steps for the data "recipe"
recipe <-
  # provide data and formula
  recipes::recipe(Value ~ ., data = xg.train) %>%
  # convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %>% 
  prep()

```


```{r echo=TRUE}

# set seed for reproducability
set.seed(123456)
# create folds for cross validation
cv_folds <- 
  # treat data using recipe
  recipes::bake(recipe,new_data = xg.train) %>%
  # specify num_folds
  rsample::vfold_cv(v = 5)

```

```{r echo=TRUE}

# XGBoost model specification using parsnip package, boost_tree function
model.XG <- 
  parsnip::boost_tree(
    mode = "regression",
    trees = tune(), # number of trees to build during model fitting
    min_n = tune(), # min number of data points required for split in tree node
    tree_depth = tune(), # max splits for trees
    learn_rate = tune(), # rate at which model minimizes loss function
    loss_reduction = tune(), # min reduction in loss function required for split
    sample_size = 0.5,
    stop_iter = 10 # stop tuning if no improvement after 10 iterations
  ) %>%
    # specify XGBoost algorithm as engine 
    set_engine("xgboost", objective = "reg:squarederror")

```


```{r echo=TRUE}

# specify parameters for grid search
xgboost_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    learn_rate(),
    loss_reduction(),
    trees()
  )
# set grid space
xgboost_grid <- 
  dials::grid_max_entropy(
    xgboost_params, 
    size = 100 # (5 CV folds) * (size = 100), 500 parameter combinations
  )
# print head for search grid
head(xgboost_grid)

```


```{r echo=TRUE}

# define the workflow
xgboost_wf <- 
  workflows::workflow() %>%
  add_model(model.XG) %>% # add the XGBoost model defined with parsnip
  add_formula(Value ~ .) # specify formula

```

```{r echo=TRUE}

# tune the model using tidymodels tune()
xgboost_tuned <- tune::tune_grid(
  object = xgboost_wf, # specified workflow
  resamples = cv_folds, # specified cross-validation
  grid = xgboost_grid, # specified search grid
  metrics = yardstick::metric_set(rmse), # metric for tuning rmse 
  control = tune::control_grid(verbose = TRUE)
)

```


```{r echo=TRUE}

# select and view best parameters from model tuning
best_params <- xgboost_tuned %>%
  tune::select_best("rmse")
# print best params
best_params

```

```{r echo=TRUE}

# finalize the model
model.best <- model.XG %>% finalize_model(best_params)

```


```{r echo=TRUE}

# fit the best model on the training data
train <- bake(recipe, new_data = xg.train)
model.best.fit <- model.best %>% fit(formula = Value ~ ., data = train)

```



```{r echo=TRUE}

# vislual assessment of model.best.fit
ggplot(model.best.fit$fit$evaluation_log) +
  geom_line(aes(iter, training_rmse), color = "red")

```

```{r echo=TRUE}

# plot variable importance
vip(model.best.fit, include_type= TRUE)

```


```{r echo=TRUE}

# make prediction on training data
predict(model.best.fit, train)%>%
bind_cols(xg.train)%>%
yardstick::metrics(Value, .pred)

```

```{r echo=TRUE}

# make a prediction on the test data
test <- bake(recipe, new_data = xg.test)
predict(model.best.fit, test)%>%
bind_cols(xg.test)%>%
yardstick::metrics(Value, .pred)

```

