model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
# Chunk 3
plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))
points(1:50,
1-pexp(1:50,rate=1/exp(model$coefficients[1])),
type="l",
lty=1)
# The survival curve S(t) for group B.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
type="l",
lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
type="l",
lty=5)
setwd("~/OneDrive - University of Denver/COMP4442/ProblemSetsWorkingDirectory/Week8")
getwd()
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# Your code to get the test data into the proper form to compute predicted values using the coefficients associated with the lambda.min as fitted on the training set
Xtest <- model.matrix(medv ~., data=dat.test)
Xtest <- Xtest[,-1]
Ytest<-dat.test$medv
# Your code to obtain the mean squared prediction error
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
cv.house.pred
MSPE <- mean((cv.house.pred-dat.test$medv)^2)
MSPE
dat.test$medv
Ytest
MSPE <- mean((cv.house.pred-Ytest)^2)
cv.house.pred
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# Your code to get the test data into the proper form to compute predicted values using the coefficients associated with the lambda.min as fitted on the training set
Xtest <- model.matrix(medv ~., data=dat.test)
Xtest <- Xtest[,-1]
Ytest<-dat.test$medv
# Your code to obtain the mean squared prediction error
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
View(cv.house.pred)
View(cv.house.pred)
mean(dat.test$medv)
MSPE
# Your code to get the test data into the proper form to compute predicted values using the coefficients associated with the lambda.min as fitted on the training set
Xtest <- model.matrix(medv ~., data=dat.test)
Xtest <- Xtest[,-1]
Ytest<-dat.test$medv
# Your code to obtain the mean squared prediction error
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
MSPE <- mean((cv.house.pred-Ytest)^2)
MSPE
mean(dat.test$medv)
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# create matrix of predictors
Xtest <- model.matrix(medv ~., data=dat.test)
# drop intercept
Xtest <- Xtest[,-1]
# vector of outcomes for prediction
Ytest<-dat.test$medv
# create prediction
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
# calculate and print MSPE
MSPE <- mean((cv.house.pred-Ytest)^2)
MSPE
# load the data
bike<-read.csv("NYCBikes.csv")
# remove the commas from M_bridge_count
bike$M_bridge_count = gsub("[\\,]", "", bike$M_bridge_count)
# print head to examine results
head(bike$M_bridge_count)
# Code to re-type your cleaned M_bridge_count variable
bike$M_bridge_count <- as.numeric(bike$M_bridge_count)
# Display with the str function to verify that variables are correctly typed
str(bike)
# drop unnecessary variables
bike.dat <- dplyr::select(bike, temp_hi, precipitation, M_bridge_count)
# check data structure
str(bike.dat)
# create the Poisson Regrssion model
model.poisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='poisson')
# print model summary
summary(model.poisson)
# create the Quasipoisson Regression model
model.quasipoisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='quasipoisson')
# print the model summary
summary(model.quasipoisson)
# create the Negative Binomial model
model.nb <- glm.nb(M_bridge_count~temp_hi+precipitation, data=bike.dat)
# print model summary
summary(model.nb)
# Chunk 1
sheep<-read.csv("sheep.deaths.csv")
with(sheep,plot(survfit(Surv(death,status)~group),lty=c(1,3,5),xlab="Age at Death (months)"))
legend("topright", c("A", "B","C"), lty = c(1,3,5))
# Chunk 2
model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
# Chunk 3
plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))
points(1:50,
1-pexp(1:50,rate=1/exp(model$coefficients[1])),
type="l",
lty=1)
# The survival curve S(t) for group B.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
type="l",
lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
type="l",
lty=5)
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# create matrix of predictors
Xtest <- model.matrix(medv ~., data=dat.test)
# drop intercept
Xtest <- Xtest[,-1]
# vector of outcomes for prediction
Ytest<-dat.test$medv
# create prediction
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
# calculate and print MSPE
MSPE <- mean((cv.house.pred-Ytest)^2)
MSPE
# load the data
bike<-read.csv("NYCBikes.csv")
# remove the commas from M_bridge_count
bike$M_bridge_count = gsub("[\\,]", "", bike$M_bridge_count)
# print head to examine results
head(bike$M_bridge_count)
# Code to re-type your cleaned M_bridge_count variable
bike$M_bridge_count <- as.numeric(bike$M_bridge_count)
# Display with the str function to verify that variables are correctly typed
str(bike)
# drop unnecessary variables
bike.dat <- dplyr::select(bike, temp_hi, precipitation, M_bridge_count)
# check data structure
str(bike.dat)
# create the Poisson Regrssion model
model.poisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='poisson')
# print model summary
summary(model.poisson)
# create the Quasipoisson Regression model
model.quasipoisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='quasipoisson')
# print the model summary
summary(model.quasipoisson)
# create the Negative Binomial model
model.nb <- glm.nb(M_bridge_count~temp_hi+precipitation, data=bike.dat)
# print model summary
summary(model.nb)
# Chunk 1
sheep<-read.csv("sheep.deaths.csv")
with(sheep,plot(survfit(Surv(death,status)~group),lty=c(1,3,5),xlab="Age at Death (months)"))
legend("topright", c("A", "B","C"), lty = c(1,3,5))
# Chunk 2
model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
# Chunk 3
plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))
points(1:50,
1-pexp(1:50,rate=1/exp(model$coefficients[1])),
type="l",
lty=1)
# The survival curve S(t) for group B.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
type="l",
lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
type="l",
lty=5)
# Load any packages, if any, that you use as part of your answers here
# For example:
library(MASS)
library(glmnet)
library(mlbench)
library(survival)
library(survminer)
data(BostonHousing) # loads the BostonHousing dataset into memory from the mlbench package
str(BostonHousing)
# get the number of rows
n <- nrow(BostonHousing)
# set seed for reproducability
set.seed(123456)
tvt2.rep <- rep(0:2,c(round(n*.2),round(n*.2),n-2*round(n*.2))) # The .2 in this function produces a 80% train/20% validation/20% test split in the data
tvt2.rep # Shows the result in the console window
table(tvt2.rep) # Shows a count of the 0's (test), 1's (valid), and 2's (train)
# Here is some room for you to change things and test how they work
# tvt3.rep for testing purposes
tvt3.rep <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# table tvt3.rep
table(tvt3.rep)
# set seed for reproducability
set.seed(123456)
# create random sample of int 0-1 from n, proportions .70, .30
tv.split <- sample(rep(0:1,c(round(n*.3),n-round(n*.3))),n)
# print table of counts (0, 1) for verification
table(tv.split)
# training set (352)
dat.train <- BostonHousing[tv.split==1,]
# test set (152)
dat.test <- BostonHousing[tv.split==0,]
# create the matrix of explanatory variables
X <- model.matrix(medv ~., data=dat.train)
# drop the intercept column
X <- X[,-1]
# create the vector of outcomes
Y<-dat.train$medv
# set seed for reproducability
set.seed(123456)
# create cross validated ridge regression model (alpha = 0 for ridge)
cvfit.house.ridge <- cv.glmnet(x=X, y=Y,alpha=0)
# print the smallest lambda for the fitted model
cvfit.house.ridge$lambda.min
# print coefficients associated with smallest lambda
coef(cvfit.house.ridge, s = "lambda.min")
# create matrix of predictors
Xtest <- model.matrix(medv ~., data=dat.test)
# drop intercept
Xtest <- Xtest[,-1]
# vector of outcomes for prediction
Ytest<-dat.test$medv
# create prediction
cv.house.pred<-predict(cvfit.house.ridge,Xtest,c(cvfit.house.ridge$lambda.min))
# calculate and print MSPE
MSPE <- mean((cv.house.pred-Ytest)^2)
MSPE
# load the data
bike<-read.csv("NYCBikes.csv")
# remove the commas from M_bridge_count
bike$M_bridge_count = gsub("[\\,]", "", bike$M_bridge_count)
# print head to examine results
head(bike$M_bridge_count)
# Code to re-type your cleaned M_bridge_count variable
bike$M_bridge_count <- as.numeric(bike$M_bridge_count)
# Display with the str function to verify that variables are correctly typed
str(bike)
# drop unnecessary variables
bike.dat <- dplyr::select(bike, temp_hi, precipitation, M_bridge_count)
# check data structure
str(bike.dat)
# create the Poisson Regrssion model
model.poisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='poisson')
# print model summary
summary(model.poisson)
# create the Quasipoisson Regression model
model.quasipoisson <- glm(M_bridge_count~temp_hi+precipitation, data=bike.dat,
family ='quasipoisson')
# print the model summary
summary(model.quasipoisson)
# create the Negative Binomial model
model.nb <- glm.nb(M_bridge_count~temp_hi+precipitation, data=bike.dat)
# print model summary
summary(model.nb)
# Chunk 1
sheep<-read.csv("sheep.deaths.csv")
with(sheep,plot(survfit(Surv(death,status)~group),lty=c(1,3,5),xlab="Age at Death (months)"))
legend("topright", c("A", "B","C"), lty = c(1,3,5))
# Chunk 2
model<-survreg(Surv(death,status)~group, dist="exponential",data=sheep)
summary(model)
# Chunk 3
plot(survfit(Surv(sheep$death,sheep$status)~sheep$group),lty=c(1,3,5),xlab="Age at Death (months)")
legend("topright", c("A", "B","C"), lty = c(1,3,5))
points(1:50,
1-pexp(1:50,rate=1/exp(model$coefficients[1])),
type="l",
lty=1)
# The survival curve S(t) for group B.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,2)]))),
type="l",
lty=3)
# The survival curve S(t) for group C.
points(1:50,
1-pexp(1:50,rate=1/exp(sum(model$coefficients[c(1,3)]))),
type="l",
lty=5)
