{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization cont.\n",
    "Last time we finished with some text normalization activities like stemming and normalization(removing [inflectional](https://en.wikipedia.org/wiki/Inflection) affixes**(ed, ing, ize, s, de)**).\n",
    "\n",
    "Note that \n",
    "\n",
    "- stemming can result in a word not in the dictionary.\n",
    "- Lemmatization ensures word is in dictionary.\n",
    "- stemming is a fast process compared to lemmatization.\n",
    "\n",
    "\n",
    "We can use use both the techniques to further reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'muse'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "word= 'muses'\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mus\n",
      "mu\n"
     ]
    }
   ],
   "source": [
    "wn_lm= WordNetLemmatizer()\n",
    "lm_word = wn_lm.lemmatize(word)\n",
    "print(lm_word)\n",
    "print(ps.stem(lm_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expanding contractions\n",
    "\n",
    "This activity invloves replacing contractions with full words like\n",
    "- can't with cannot.\n",
    "- Should've with should have\n",
    "- Weren't were not\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Any suggestion how to do it? What module and function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_patterns=[(r'can\\'t', 'cannot'),\n",
    "                    (r'haven\\'t', 'have not'),\n",
    "                    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "                    (r'(\\w+)\\'re', '\\g<1> are')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class contraction_replacer(object):\n",
    "    def __init__(self, contraction_patterns):        \n",
    "        # store compiled regex object\n",
    "        self._contraction_regexes = [(????, replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self._contraction_regexes:\n",
    "            text = contraction_regex.???\n",
    "        return text     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Let's use it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_contraction_replacer = contraction_replacer(contraction_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We will do this work'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_contraction_replacer.do_contraction_normalization(\"We'll do this work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'will', 'do', 'this', 'work']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing contraction and tokenize\n",
    "nltk.tokenize.word_tokenize(sample_contraction_replacer.do_contraction_normalization(\"We'll do this work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Removing repeated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):       \n",
    "        \n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        compressed_word = ???\n",
    "        \n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Notice how backreferences(\\1, \\2, \\3) are used\n",
    "sample_repeat_replacer = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)',  ???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oh', 'love')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('ooooh'), sample_repeat_replacer.do_repeat_normalization('loooove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens when word has repeating character!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shep'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_repeat_replacer.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "class repeat_replacer(object):\n",
    "    def __init__(self, repeat_patterns, sub_pattern):\n",
    "        \n",
    "        \n",
    "        # store compiled regex object\n",
    "        self._repeat_regexes = re.compile(repeat_patterns)\n",
    "        self._sub_pattern = sub_pattern\n",
    "    def do_repeat_normalization(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        compressed_word = self._repeat_regexes.sub(self._sub_pattern, word)\n",
    "        if compressed_word != word:\n",
    "            #print('iside if')\n",
    "            compressed_word = self.do_repeat_normalization(compressed_word)\n",
    "        return compressed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "repeat_replacer_inst = repeat_replacer(r'(\\w*)(\\w)\\2(\\w*)', r'\\1\\2\\3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sheep'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_replacer_inst.do_repeat_normalization('sheep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spelling correction with Enchant\n",
    "\n",
    "Go to \n",
    "http://www.abisource.com/projects/enchant/ \n",
    "to learn more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's build a spell checker class, We need\n",
    "- a spellchecking library like enchant. We just installed it\n",
    "- and a dictionary for it to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# aspell demo at command prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let' see how enchant works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('af', <Enchant: Aspell Provider>),\n",
       " ('am', <Enchant: Aspell Provider>),\n",
       " ('ar', <Enchant: Aspell Provider>),\n",
       " ('ast', <Enchant: Aspell Provider>),\n",
       " ('az', <Enchant: Aspell Provider>),\n",
       " ('be', <Enchant: Aspell Provider>),\n",
       " ('be_BY', <Enchant: Aspell Provider>),\n",
       " ('be_SU', <Enchant: Aspell Provider>),\n",
       " ('bg', <Enchant: Aspell Provider>),\n",
       " ('bn', <Enchant: Aspell Provider>),\n",
       " ('br', <Enchant: Aspell Provider>),\n",
       " ('ca', <Enchant: Aspell Provider>),\n",
       " ('cs', <Enchant: Aspell Provider>),\n",
       " ('csb', <Enchant: Aspell Provider>),\n",
       " ('cy', <Enchant: Aspell Provider>),\n",
       " ('da', <Enchant: Aspell Provider>),\n",
       " ('de', <Enchant: Aspell Provider>),\n",
       " ('de_AT', <Enchant: Aspell Provider>),\n",
       " ('de_CH', <Enchant: Aspell Provider>),\n",
       " ('de_DE', <Enchant: Aspell Provider>),\n",
       " ('el', <Enchant: Aspell Provider>),\n",
       " ('en', <Enchant: Aspell Provider>),\n",
       " ('en_AU', <Enchant: Aspell Provider>),\n",
       " ('en_CA', <Enchant: Aspell Provider>),\n",
       " ('en_GB', <Enchant: Aspell Provider>),\n",
       " ('en_US', <Enchant: Aspell Provider>),\n",
       " ('eo', <Enchant: Aspell Provider>),\n",
       " ('es', <Enchant: Aspell Provider>),\n",
       " ('es_ES', <Enchant: AppleSpell Provider>),\n",
       " ('et', <Enchant: Aspell Provider>),\n",
       " ('fa', <Enchant: Aspell Provider>),\n",
       " ('fi', <Enchant: Aspell Provider>),\n",
       " ('fo', <Enchant: Aspell Provider>),\n",
       " ('fr', <Enchant: Aspell Provider>),\n",
       " ('fr_CH', <Enchant: Aspell Provider>),\n",
       " ('fr_FR', <Enchant: Aspell Provider>),\n",
       " ('fy', <Enchant: Aspell Provider>),\n",
       " ('ga', <Enchant: Aspell Provider>),\n",
       " ('gd', <Enchant: Aspell Provider>),\n",
       " ('gl', <Enchant: Aspell Provider>),\n",
       " ('gr', <Enchant: Aspell Provider>),\n",
       " ('grc', <Enchant: Aspell Provider>),\n",
       " ('gu', <Enchant: Aspell Provider>),\n",
       " ('gv', <Enchant: Aspell Provider>),\n",
       " ('he', <Enchant: Aspell Provider>),\n",
       " ('hi', <Enchant: Aspell Provider>),\n",
       " ('hil', <Enchant: Aspell Provider>),\n",
       " ('hr', <Enchant: Aspell Provider>),\n",
       " ('hsb', <Enchant: Aspell Provider>),\n",
       " ('hu', <Enchant: Aspell Provider>),\n",
       " ('hu_HU', <Enchant: AppleSpell Provider>),\n",
       " ('hus', <Enchant: Aspell Provider>),\n",
       " ('hy', <Enchant: Aspell Provider>),\n",
       " ('ia', <Enchant: Aspell Provider>),\n",
       " ('id', <Enchant: Aspell Provider>),\n",
       " ('it', <Enchant: Aspell Provider>),\n",
       " ('it_IT', <Enchant: AppleSpell Provider>),\n",
       " ('kn', <Enchant: Aspell Provider>),\n",
       " ('ku', <Enchant: Aspell Provider>),\n",
       " ('ky', <Enchant: Aspell Provider>),\n",
       " ('la', <Enchant: Aspell Provider>),\n",
       " ('lt', <Enchant: Aspell Provider>),\n",
       " ('lv', <Enchant: Aspell Provider>),\n",
       " ('mg', <Enchant: Aspell Provider>),\n",
       " ('mi', <Enchant: Aspell Provider>),\n",
       " ('mk', <Enchant: Aspell Provider>),\n",
       " ('ml', <Enchant: Aspell Provider>),\n",
       " ('mn', <Enchant: Aspell Provider>),\n",
       " ('mr', <Enchant: Aspell Provider>),\n",
       " ('ms', <Enchant: Aspell Provider>),\n",
       " ('mt', <Enchant: Aspell Provider>),\n",
       " ('nds', <Enchant: Aspell Provider>),\n",
       " ('nl', <Enchant: Aspell Provider>),\n",
       " ('nl_NL', <Enchant: AppleSpell Provider>),\n",
       " ('nn', <Enchant: Aspell Provider>),\n",
       " ('ny', <Enchant: Aspell Provider>),\n",
       " ('or', <Enchant: Aspell Provider>),\n",
       " ('pa', <Enchant: Aspell Provider>),\n",
       " ('pl', <Enchant: Aspell Provider>),\n",
       " ('pt_BR', <Enchant: Aspell Provider>),\n",
       " ('pt_PT', <Enchant: Aspell Provider>),\n",
       " ('qu', <Enchant: Aspell Provider>),\n",
       " ('ro', <Enchant: Aspell Provider>),\n",
       " ('ru', <Enchant: Aspell Provider>),\n",
       " ('rw', <Enchant: Aspell Provider>),\n",
       " ('sc', <Enchant: Aspell Provider>),\n",
       " ('sk', <Enchant: Aspell Provider>),\n",
       " ('sk_SK', <Enchant: Aspell Provider>),\n",
       " ('sl', <Enchant: Aspell Provider>),\n",
       " ('sr', <Enchant: Aspell Provider>),\n",
       " ('srd', <Enchant: Aspell Provider>),\n",
       " ('sv', <Enchant: Aspell Provider>),\n",
       " ('sv_SE', <Enchant: AppleSpell Provider>),\n",
       " ('sw', <Enchant: Aspell Provider>),\n",
       " ('ta', <Enchant: Aspell Provider>),\n",
       " ('te', <Enchant: Aspell Provider>),\n",
       " ('tet', <Enchant: Aspell Provider>),\n",
       " ('tk', <Enchant: Aspell Provider>),\n",
       " ('tl', <Enchant: Aspell Provider>),\n",
       " ('tn', <Enchant: Aspell Provider>),\n",
       " ('tr', <Enchant: Aspell Provider>),\n",
       " ('uk', <Enchant: Aspell Provider>),\n",
       " ('uz', <Enchant: Aspell Provider>),\n",
       " ('vi', <Enchant: Aspell Provider>),\n",
       " ('wa', <Enchant: Aspell Provider>),\n",
       " ('yi', <Enchant: Aspell Provider>),\n",
       " ('zu', <Enchant: Aspell Provider>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "enchant.list_dicts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_int = enchant.Dict('en')\n",
    "dict_int.check('love'), dict_int.check('lov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scion',\n",
       " 'cine',\n",
       " 'sine',\n",
       " 'scene',\n",
       " 'Seine',\n",
       " 'Sen',\n",
       " 'seine',\n",
       " 'sen',\n",
       " 'sin',\n",
       " 'seen',\n",
       " 'sign',\n",
       " 'sci en',\n",
       " 'sci-en',\n",
       " 'scone',\n",
       " 'sienna',\n",
       " 'Sean',\n",
       " 'seeing',\n",
       " 'sewn',\n",
       " 'sing',\n",
       " 'sinew',\n",
       " 'scent',\n",
       " 'siren',\n",
       " 'Sn',\n",
       " 'sane',\n",
       " 'sicken',\n",
       " 'zine',\n",
       " 'Stein',\n",
       " 'Stine',\n",
       " 'sci',\n",
       " 'science',\n",
       " 'skein',\n",
       " 'spine',\n",
       " 'stein',\n",
       " 'swine',\n",
       " 'seiner']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_int.suggest('scien')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How edit distance works\n",
    "\n",
    "*minimum number of character changes to transform one word into another.*\n",
    "\n",
    "See wiki for details\n",
    "https://en.wikipedia.org/wiki/Edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('sciena', 'science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's write the class for performing spell correction\n",
    "- import enchant and initialize a dictionary(will use opensource aspell http://aspell.net/) for it to use\n",
    "- import edit_distance  from nltk.metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "class spell_checker(object):\n",
    "    def __init__(self, dict_name='en_US', max_edit_dist=3):\n",
    "        self._dict= enchant.Dict(dict_name)\n",
    "        self._max_edit_dist = max_edit_dist\n",
    "    def _word_with_min_dist(self, word, suggestions):\n",
    "        print(suggestions)\n",
    "        #min_edit_distance = np.inf\n",
    "        corrected_word = word\n",
    "        for sug in [suggestions[0]]:\n",
    "            distance = edit_distance(word, sug)\n",
    "            #print(distance)\n",
    "            if distance < self._max_edit_dist:\n",
    "                print(distance, sug)\n",
    "                min_edit_distance = distance\n",
    "                corrected_word = sug\n",
    "        return corrected_word        \n",
    "                \n",
    "                \n",
    "        \n",
    "    def check_spell(self, word):\n",
    "        if self._dict.check(word):\n",
    "            return word\n",
    "        # the the words with minimum distance\n",
    "        return self._word_with_min_dist(word, self._dict.suggest(word))\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "spell_check_int = spell_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jukebox', 'Quebec', 'cubic', 'jujube', 'kickback', 'jujubes', \"jujube's\", 'jojoba', 'Jacob', 'KGB', 'jackboot', 'Jacobi', 'cookbook', 'Jacobs', \"KGB's\", \"Jacob's\"]\n",
      "1 jukebox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jukebox'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_int.check_spell('jukeboc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use right dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theater'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_spell_ckeck_inst = spell_checker('en_US')\n",
    "us_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theatre', 'heater', 'cheater', 'theta', 'that', 'eater', 'hater', 'tater', 'threader', 'beater', 'header', 'neater', 'teeter', 'Theiler']\n",
      "2 theatre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'theatre'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br_spell_ckeck_inst = spell_checker('en_GB')\n",
    "br_spell_ckeck_inst.check_spell('theater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Adding custom word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'echo -e \"deeplearning\\\\nnlp\" > my_words.txxxt\\ncat my_words.txxxt\\n'' died with <Signals.SIGKILL: 9>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb Cell 39'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000039?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mecho -e \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdeeplearning\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mnnlp\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m > my_words.txxxt\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mcat my_words.txxxt\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2357\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2354'>2355</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2355'>2356</a>\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2356'>2357</a>\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2357'>2358</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=150'>151</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=151'>152</a>\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=152'>153</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=299'>300</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=300'>301</a>\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=301'>302</a>\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=302'>303</a>\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=303'>304</a>\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=304'>305</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'echo -e \"deeplearning\\\\nnlp\" > my_words.txxxt\\ncat my_words.txxxt\\n'' died with <Signals.SIGKILL: 9>."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"deeplearning\\nnlp\" > my_words.txxxt\n",
    "cat my_words.txxxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = enchant.Dict('en_US')\n",
    "d1.check('nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = enchant.DictWithPWL ('en_US', 'my_words.txxxt')\n",
    "d2.check('nlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class synonynm(object):\n",
    "    def __init__(self, word_map):\n",
    "        self._map = word_map\n",
    "    def get_synonym(self, word):\n",
    "        return self._map.get(word, word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "synonynm_inst = synonynm({'bday': 'birthday', 'yolo':'you live only once'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you live only once'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonynm_inst.get_synonym('yolo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "we could have maintained a dictionary but this solution is not a extensible solution. One can maintain synonym dictionary in any format and synonym class can acts a wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000047?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000047?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mcsv_based_synonym\u001b[39;00m(synonynm):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000047?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, file_name):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "class csv_based_synonym(synonynm):\n",
    "    def __init__(self, file_name):\n",
    "        word_map = {}\n",
    "        for line in file_name:\n",
    "            word, syn = line\n",
    "            word_map[word] = syn\n",
    "        super(csv_based_synonym ,self).__init__(word_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"# Let's create a csv file\\necho -e 'hpy, happy\\\\nbday, birthday' > syn.csv\\ncat syn.csv\\n\"' died with <Signals.SIGKILL: 9>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb Cell 48'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000048?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m# Let\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms create a csv file\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mecho -e \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhpy, happy\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mnbday, birthday\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m > syn.csv\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mcat syn.csv\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2357\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2354'>2355</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2355'>2356</a>\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2356'>2357</a>\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/interactiveshell.py?line=2357'>2358</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=150'>151</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=151'>152</a>\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=152'>153</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=299'>300</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=300'>301</a>\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=301'>302</a>\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=302'>303</a>\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=303'>304</a>\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/benkarabinus/.local/share/virtualenvs/COMP4447-vIIoMJZg/lib/python3.9/site-packages/IPython/core/magics/script.py?line=304'>305</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"# Let's create a csv file\\necho -e 'hpy, happy\\\\nbday, birthday' > syn.csv\\ncat syn.csv\\n\"' died with <Signals.SIGKILL: 9>."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Let's create a csv file\n",
    "echo -e 'hpy, happy\\nbday, birthday' > syn.csv\n",
    "cat syn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_based_synonym_int = csv_based_synonym('syn.csv')\n",
    "csv_based_synonym_int.get_synonym('hpy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# review of WordNet(a lexical database for the English language)\n",
    "\n",
    "nltk provides an interface to WordNet synset(synonymous words that express the same concept.) lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('science.n.01'), Synset('skill.n.02')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n",
      "rise.n.09\n",
      "an increase in cost\n",
      "raise.n.01\n",
      "the amount a salary is increased\n",
      "hike.v.01\n",
      "increase\n",
      "hike.v.02\n",
      "walk a long way, as for pleasure or physical exercise\n"
     ]
    }
   ],
   "source": [
    "for syn in wordnet.synsets('hike'):\n",
    "    print(syn.name())\n",
    "    print(syn.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Looking for lemmas and synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synset' object has no attribute 'synonyms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000056?line=0'>1</a>\u001b[0m \u001b[39m# Let's take first synset for science\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000056?line=1'>2</a>\u001b[0m syn \u001b[39m=\u001b[39m wordnet\u001b[39m.\u001b[39msynsets(\u001b[39m'\u001b[39m\u001b[39mscience\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benkarabinus/Git/nerdy_data_stuff/DU/COMP4447/Week6/nlp_25_feb_cleaning.ipynb#ch0000056?line=2'>3</a>\u001b[0m syn\u001b[39m.\u001b[39;49msynonyms\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Synset' object has no attribute 'synonyms'"
     ]
    }
   ],
   "source": [
    "# Let's take first synset for science\n",
    "syn = wordnet.synsets('science')[0]\n",
    "syn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('science.n.01.science'), Lemma('science.n.01.scientific_discipline')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science', 'scientific_discipline']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can treat lemmas as synonyms\n",
    "[l.name() for l in syn.lemmas()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Antonyms\n",
    "lemmas has antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('gladiolus.n.01'),\n",
       " Synset('glad.a.01'),\n",
       " Synset('glad.s.02'),\n",
       " Synset('glad.s.03'),\n",
       " Synset('beaming.s.01')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('glad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'showing or causing joy and pleasure; especially made happy'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('glad')[1]\n",
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('sad.a.01.sad')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms = syn.lemmas()[0].antonyms()\n",
    "glad_antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiencing or showing sorrow or unhappiness'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glad_antonyms[0].synset().definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back to replacing negation with antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class antonym_replacer(object):\n",
    "    def _find_antonym(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "        ????finish code here\n",
    "    def remove_negation(self, sent):\n",
    "        s=0\n",
    "        l=len(sent)\n",
    "        clean_words= []\n",
    "        while s < l -1:\n",
    "            possible_not_word = sent[s]\n",
    "            word = sent[s +1 ]\n",
    "            if possible_not_word == 'not':\n",
    "                ant= self._find_antonym(word)\n",
    "                print(ant)\n",
    "                if ant:\n",
    "                    clean_words.append(ant)\n",
    "                    s+=2\n",
    "            else:\n",
    "                clean_words.append(possible_not_word)\n",
    "                \n",
    "                if s==l-2:\n",
    "                    clean_words.append(word)\n",
    "                s+=1\n",
    "            \n",
    "        return clean_words    \n",
    "                                     \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'not', 'uglify', 'this', 'place']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Let's not uglify this place\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'not', 'uglify', 'this', 'code']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we want Let's together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautify\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'beautify', 'this', 'place']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym_replacer_inst= antonym_replacer()\n",
    "antonym_replacer_inst.remove_negation(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Side: WordNet Methods you may find useful for your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Synstes are organised in the form of a tree using **hypernyms** and **hyponyms**\n",
    "\n",
    "**hypernyms:** abstract terms are known as hypernyms\n",
    "\n",
    "**hyponyms:** more specific terms as hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hike.n.01\n",
      "a long walk usually for exercise or pleasure\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Synset('walk.n.04')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synsets('hike')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())\n",
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('amble.n.01'),\n",
       " Synset('constitutional.n.01'),\n",
       " Synset('foot.n.07'),\n",
       " Synset('hike.n.01'),\n",
       " Synset('last_mile.n.01'),\n",
       " Synset('moonwalk.n.02'),\n",
       " Synset('perambulation.n.01'),\n",
       " Synset('turn.n.12'),\n",
       " Synset('walk-through.n.04'),\n",
       " Synset('walkabout.n.03')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('abstraction.n.06'),\n",
       "  Synset('psychological_feature.n.01'),\n",
       "  Synset('event.n.01'),\n",
       "  Synset('act.n.02'),\n",
       "  Synset('action.n.01'),\n",
       "  Synset('change.n.03'),\n",
       "  Synset('motion.n.06'),\n",
       "  Synset('travel.n.01'),\n",
       "  Synset('walk.n.04'),\n",
       "  Synset('hike.n.01')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Finding similarity\n",
    "Using hypernym tree for similarity between the Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wu-Palmer Similarity\n",
    "\n",
    "It is based on how similar the word senses are and realtive position of synsets in hypernym tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('faux_pas.n.01'),\n",
       " Synset('slip.n.02'),\n",
       " Synset('slip.n.03'),\n",
       " Synset('cutting.n.02'),\n",
       " Synset('slip.n.05'),\n",
       " Synset('mooring.n.01'),\n",
       " Synset('slip.n.07'),\n",
       " Synset('slickness.n.03'),\n",
       " Synset('strip.n.02'),\n",
       " Synset('slip.n.10'),\n",
       " Synset('chemise.n.01'),\n",
       " Synset('case.n.19'),\n",
       " Synset('skid.n.03'),\n",
       " Synset('slip.n.14'),\n",
       " Synset('slip.n.15'),\n",
       " Synset('steal.v.02'),\n",
       " Synset('slip.v.02'),\n",
       " Synset('skid.v.04'),\n",
       " Synset('slip.v.04'),\n",
       " Synset('slip.v.05'),\n",
       " Synset('err.v.01'),\n",
       " Synset('slip.v.07'),\n",
       " Synset('slip.v.08'),\n",
       " Synset('slip.v.09'),\n",
       " Synset('slip.v.10'),\n",
       " Synset('dislocate.v.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets('slip')\n",
    "syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], syns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11764705882352941"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.wup_similarity(syns[0], wordnet.synsets('apple')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above metric uses shortest path distance between the two Synsets and their common hypernym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "d1 = syns[0]\n",
    "d2 = syns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.shortest_path_distance(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('blunder.n.01')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1 = d1.hypernyms()[0]\n",
    "common_hypernym_d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d1.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('mistake.n.01')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2 = d2.hypernyms()[0]\n",
    "common_hypernym_d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_hypernym_d2.shortest_path_distance(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Be care comapring verbs, as many verbs don't share common hypernyms. Return value would be None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# read nltk for word collocation\n",
    "\n",
    "- http://www.nltk.org/howto/collocations.html\n",
    "- https://www.nltk.org/\n",
    "\n",
    "- https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# NLP Resources\n",
    "https://web.stanford.edu/~jurafsky/slp3/\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/\n",
    "\n",
    "1 - Ruder website: http://ruder.io/ (all his tutorials are amazing, I suggest you to start from old posts he has on the website)\n",
    "\n",
    "2 - https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8416973&tag=1 (this covers the most recent advances in DL in NLP)\n",
    "\n",
    "3 - pretty much everything in this website: https://machinelearningmastery.com/category/natural-language-processing/\n",
    "\n",
    "4 - This github repo has a lot of good resources: https://github.com/keon/awesome-nlp\n",
    "\n",
    "5- https://www.youtube.com/watch?v=jfwqRMdTmLo&list=\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
